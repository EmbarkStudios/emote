{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f0081a-2445-4a07-97eb-155c01ea0a9c",
   "metadata": {},
   "source": [
    "# Transformer Tests\n",
    "\n",
    "Herein we try to figure out how transformers could work for our vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9c78c5-0789-48ec-a1cd-c6a40eb0a293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from perlin_numpy import generate_perlin_noise_2d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ef9761-7ce1-4675-8ad7-74467d8a9f07",
   "metadata": {},
   "source": [
    "## Data creation\n",
    "\n",
    "The goal of the transformer is to recreate a bit of terrain based on a few raycasts. Each sample in our dataset will therefore be of the format\n",
    "$$\n",
    "\\left( P \\oplus N \\right)^K \\to H : (\\mathbf p_1, \\mathbf n_1, \\ldots, \\mathbf p_K, \\mathbf n_K) \\mapsto \\mathbf h\n",
    "$$\n",
    "where $P$ represents ray trace hits, $N$ normals at the point the ray hit, we have $K$ such rays, and $H$ is the space of local height fields around the agent.\n",
    "\n",
    "For $H$ we will start by creating a bunch of synthetic heightmap data using a perlin noise generator. For each example it will generate a 30x30 grid of perlin noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d226e3b-59c9-4157-b4cb-d4d0252899e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 2\n",
    "image_size = 10//res*res\n",
    "pixel_size = 0.2 # Each pixel is 20 cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88a4f20-ffb0-4d2c-8520-48ee3a5d335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ground():\n",
    "    return 10 * generate_perlin_noise_2d((image_size, image_size), (res, res), (False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879285c5-b254-4df3-a7b0-cbf15ce2e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a vector - three numbers - and normalize it, i.e make it's length = 1\n",
    "def normalizeRGB(vec):\n",
    "    length = np.sqrt(vec[:,:,0]**2 + vec[:,:,1]**2 + vec[:,:,2]**2)\n",
    "    vec[:,:,0] = vec[:,:,0] / length\n",
    "    vec[:,:,1] = vec[:,:,1] / length\n",
    "    vec[:,:,2] = vec[:,:,2] / length\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efd665-96e1-499b-ad56-3ff92037cc67",
   "metadata": {},
   "source": [
    "We use a simple finite difference approximation for the normals, so just shift the image one step in each direction and then take the difference. There are better approximations available (see e.g. [this](https://bartwronski.com/2021/02/28/computing-gradients-on-grids-forward-central-and-diagonal-differences/)) but this works fine for us.\n",
    "\n",
    "We can write a height map $h$ as the surface \n",
    "$$\n",
    "f(x, y, z) = h(x, y) - z = 0.\n",
    "$$\n",
    "\n",
    "Then the normal is given by\n",
    "$$\n",
    "n(x, y, z) = \\nabla f = \\left( \\frac{\\partial h}{\\partial x},  \\frac{\\partial h}{\\partial y}, -1 \\right).\n",
    "$$\n",
    "\n",
    "Then we can approximate the partial derivatives at a given pixel with the differences between surrounding pixels, divided by twice the grid step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6013599-03a5-42e0-815c-b339ff2283e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_normals(ground):\n",
    "    bottom = np.pad(ground, 1, mode='edge')[2:, 1:-1]\n",
    "    top = np.pad(ground, 1, mode='edge')[:-2, 1:-1]\n",
    "    left = np.pad(ground, 1, mode='edge')[1:-1, 0:-2]\n",
    "    right = np.pad(ground, 1, mode='edge')[1:-1, 2:]\n",
    "    \n",
    "    xy_scale = 2/pixel_size\n",
    "    grad_x = xy_scale * (right - left)\n",
    "    grad_y = xy_scale * (bottom - top)\n",
    "    return normalizeRGB(np.array([grad_x, grad_y, -4*np.ones((image_size, image_size))]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752fd14f-2206-4165-971d-f293e73717c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x110b01870>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ground = make_ground()\n",
    "normals = make_normals(ground)\n",
    "plt.subplot(131)\n",
    "plt.imshow(ground)\n",
    "plt.subplot(132)\n",
    "plt.imshow(normals[:,:,0], \"PiYG\")\n",
    "plt.subplot(133)\n",
    "plt.imshow(normals[:,:,1], \"PiYG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5b0273-431e-485e-afa3-e68938a62b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_raytoken(ground, normals):\n",
    "    (size_x, size_y) = ground.shape\n",
    "    x_s = np.random.randint(size_x)\n",
    "    y_s = np.random.randint(size_y)\n",
    "    height = ground[x_s, y_s]\n",
    "    normal = normals[x_s, y_s]\n",
    "    relative_x = (x_s - size_x / 2) * pixel_size\n",
    "    relative_y = (y_s - size_y / 2) * pixel_size\n",
    "    th\n",
    "    return np.array([relative_x, relative_y, height, normal[0], normal[1], normal[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a863701-d8e1-47fb-adc9-d2bd56fca505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(ground, num_samples):\n",
    "    normals = make_normals(ground)\n",
    "    samples = np.array([sample_raytoken(ground, normals) for i in range(num_samples)])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76042e80-5465-4527-9fa0-d01799f09f85",
   "metadata": {},
   "source": [
    "## The Transformer part\n",
    "\n",
    "We define a simple transformer model in pytorch and train it on this synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ac6af2-2aec-42d8-b9d7-853e70574e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformer(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, sequence_length, image_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead),\n",
    "            num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model * sequence_length, image_size * image_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # Change from (batch_size, seq_len, embedding_size) to (seq_len, batch_size, embedding_size)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Change back to (batch_size, seq_len, embedding_size)\n",
    "        x = x.reshape(-1, self.sequence_length * x.size(2))\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.image_size, self.image_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0723b9b8-36f5-4617-b777-f6c177752a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we generate random input data and target images\n",
    "input_size = 2  # size of each input embedding vector\n",
    "nhead = 4  # number of attention heads in the transformer\n",
    "num_layers = 10  # number of transformer layers\n",
    "d_model = image_size * image_size  # internal model dimension\n",
    "\n",
    "seq_len = 30  # Number of ray tokens per ground image\n",
    "num_samples = 1000\n",
    "\n",
    "# Model\n",
    "model = ImageTransformer(input_size, d_model, nhead, num_layers, seq_len, image_size)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4e8b964-a6ad-44b9-8f32-29cfa8a89eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(model, optimizer, inputs, targets):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Flatten the outputs and targets for the loss calculation\n",
    "    outputs_flat = outputs.view(-1, image_size * image_size)\n",
    "    targets_flat = targets.view(-1, image_size * image_size)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs_flat, targets_flat)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss for monitoring training progress\n",
    "    print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aa78541-2026-483f-8ff0-94823c8eb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_n_blips(n):\n",
    "    ground = np.zeros((image_size, image_size))\n",
    "    blips = np.array([[np.random.randint(image_size), np.random.randint(image_size)] for _ in range(n)]).T\n",
    "    ground[blips[0], blips[1]] = 1.\n",
    "    blips = blips/image_size - 0.5\n",
    "    return ground, blips.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f37adc8-d45a-407c-a890-75aab6a6a8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Loss: 0.2364\n",
      "Epoch [1/10], Loss: 0.2421\n",
      "Epoch [2/10], Loss: 0.2281\n",
      "Epoch [3/10], Loss: 0.2311\n",
      "Epoch [4/10], Loss: 0.2348\n",
      "Epoch [5/10], Loss: 0.2234\n",
      "Epoch [6/10], Loss: 0.2263\n",
      "Epoch [7/10], Loss: 0.2299\n",
      "Epoch [8/10], Loss: 0.2225\n",
      "Epoch [9/10], Loss: 0.2232\n"
     ]
    }
   ],
   "source": [
    "# ground = make_ground()\n",
    "targets_blips = [make_n_blips(seq_len) for _ in range(num_samples)]\n",
    "targets, inputs = zip(*targets_blips)\n",
    "for epoch in range(num_epochs):\n",
    "    inputs, targets = torch.tensor(np.array(inputs)).float(), torch.tensor(np.array(targets)).float()\n",
    "    step(model, optimizer, inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2eb9c14-a650-4be1-96cb-07f2ca32c867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3000,  0.0000],\n",
       "        [-0.1000, -0.2000],\n",
       "        [ 0.0000,  0.1000],\n",
       "        [-0.1000, -0.4000],\n",
       "        [-0.5000,  0.3000],\n",
       "        [ 0.1000,  0.0000],\n",
       "        [ 0.3000,  0.4000],\n",
       "        [ 0.4000, -0.3000],\n",
       "        [ 0.1000, -0.2000],\n",
       "        [ 0.3000, -0.2000],\n",
       "        [ 0.0000,  0.3000],\n",
       "        [ 0.2000,  0.0000],\n",
       "        [ 0.1000, -0.2000],\n",
       "        [ 0.2000, -0.4000],\n",
       "        [-0.4000,  0.0000],\n",
       "        [-0.4000,  0.1000],\n",
       "        [ 0.1000,  0.2000],\n",
       "        [-0.4000, -0.2000],\n",
       "        [-0.3000,  0.3000],\n",
       "        [ 0.1000, -0.2000],\n",
       "        [ 0.2000, -0.3000],\n",
       "        [ 0.2000, -0.3000],\n",
       "        [-0.1000, -0.3000],\n",
       "        [ 0.1000,  0.3000],\n",
       "        [-0.3000, -0.3000],\n",
       "        [-0.5000,  0.3000],\n",
       "        [-0.4000, -0.5000],\n",
       "        [-0.3000,  0.2000],\n",
       "        [ 0.3000,  0.2000],\n",
       "        [-0.5000,  0.3000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56e1778c-2501-44b5-9b4f-914f4d8d6a58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'th' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m targets_blips \u001b[38;5;241m=\u001b[39m [make_n_blips(seq_len) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[1;32m      3\u001b[0m targets \u001b[38;5;241m=\u001b[39m [make_ground() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[0;32m----> 4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [sample_data(ground, seq_len) \u001b[38;5;28;01mfor\u001b[39;00m ground \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m      6\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(inputs))\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(targets))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m targets_blips \u001b[38;5;241m=\u001b[39m [make_n_blips(seq_len) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[1;32m      3\u001b[0m targets \u001b[38;5;241m=\u001b[39m [make_ground() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[0;32m----> 4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [\u001b[43msample_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ground \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m      6\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(inputs))\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(targets))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36msample_data\u001b[0;34m(ground, num_samples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_data\u001b[39m(ground, num_samples):\n\u001b[1;32m      2\u001b[0m     normals \u001b[38;5;241m=\u001b[39m make_normals(ground)\n\u001b[0;32m----> 3\u001b[0m     samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sample_raytoken(ground, normals) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_data\u001b[39m(ground, num_samples):\n\u001b[1;32m      2\u001b[0m     normals \u001b[38;5;241m=\u001b[39m make_normals(ground)\n\u001b[0;32m----> 3\u001b[0m     samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43msample_raytoken\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormals\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)])\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m samples\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36msample_raytoken\u001b[0;34m(ground, normals)\u001b[0m\n\u001b[1;32m      7\u001b[0m relative_x \u001b[38;5;241m=\u001b[39m (x_s \u001b[38;5;241m-\u001b[39m size_x \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m pixel_size\n\u001b[1;32m      8\u001b[0m relative_y \u001b[38;5;241m=\u001b[39m (y_s \u001b[38;5;241m-\u001b[39m size_y \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m pixel_size\n\u001b[0;32m----> 9\u001b[0m \u001b[43mth\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([relative_x, relative_y, height, normal[\u001b[38;5;241m0\u001b[39m], normal[\u001b[38;5;241m1\u001b[39m], normal[\u001b[38;5;241m2\u001b[39m]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'th' is not defined"
     ]
    }
   ],
   "source": [
    "# ground = make_ground()\n",
    "targets_blips = [make_n_blips(seq_len) for _ in range(num_samples)]\n",
    "targets = [make_ground() for _ in range(num_samples)]\n",
    "inputs = [sample_data(ground, seq_len) for ground in targets]\n",
    "for epoch in range(30):\n",
    "    inputs, targets = torch.tensor(np.array(inputs)).float(), torch.tensor(np.array(targets)).float()\n",
    "    step(model, optimizer, inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac21605-e087-40ce-816b-03098ce41ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(30):\n",
    "#     targets = [make_ground() for _ in range(num_samples)]\n",
    "#     inputs = [sample_data(ground, seq_len) for ground in targets]\n",
    "#     inputs, targets = torch.tensor(np.array(inputs)).float(), torch.tensor(np.array(targets)).float()\n",
    "#     step(model, optimizer, inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37143098-8128-49e9-a67f-d0a63d95f346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13dd05cf0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After training, you can use the model for inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Example inference with a new input sequence\n",
    "    # ground = make_ground()\n",
    "    ground = targets[102]\n",
    "    # input = sample_data(ground, seq_len)\n",
    "    # input = torch.tensor(np.array([input])).float()\n",
    "    input = inputs[102].unsqueeze(0)\n",
    "    predicted_ground = model(input)\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(ground)\n",
    "plt.subplot(132)\n",
    "plt.imshow(predicted_ground[0].numpy())\n",
    "plt.subplot(133)\n",
    "plt.imshow(ground - predicted_ground[0].numpy(), \"PiYG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48069a8-8f29-4154-844d-bf6a8fe79505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
