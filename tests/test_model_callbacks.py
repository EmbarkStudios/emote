import torch
from emote.models.model import DynamicModel
from emote.models.model_env import ModelEnv
from emote.models.callbacks import ModelBasedCollector
from emote.callbacks import BackPropStepsTerminator
from gymnasium.vector import AsyncVectorEnv
from emote import Trainer
from emote.sac import FeatureAgentProxy
from tests.gym import DictGymWrapper, HitTheMiddle
from tests.utils import create_memory
from emote.extra.schedules import BPStepScheduler
from tests.utils import MultiplierNN, RandomPolicy, FakeDataloader


def test_model_collector():
    """ The function tests unrolling a dynamic model and storing the rollouts in a replay buffer.
        The fake dynamic model simply multiplies the inputs by a fixed (rand) number, i.e.,
            next_obs = obs x rand_number,
            rewards = actions x rand_number.

        The test checks the following:
            * the replay buffer contains a correct number of samples,
            * stored samples are the ones generated by the fake model.

    """
    num_obs = 2
    num_actions = 1
    batch_size = 10
    rollout_size = 5
    rl_data_group = 'rl_group'
    rand_multiplier = torch.rand(1)[0] * 10
    env = DictGymWrapper(AsyncVectorEnv(2 * [HitTheMiddle]))  # dummy envs
    device = torch.device('cpu')
    model = MultiplierNN(value=rand_multiplier, device=device)
    dynamic_model = DynamicModel(model=model, no_delta_list=[0, 1])
    model_env = ModelEnv(num_envs=batch_size, model=dynamic_model, termination_fn=lambda a: torch.zeros(a.shape[0]))
    policy = RandomPolicy(action_dim=num_actions)
    agent = FeatureAgentProxy(policy, device)
    memory, dataloader = create_memory(
        env, memory_size=1000, len_rollout=1, batch_size=batch_size, data_group=rl_data_group, device=device
    )
    callbacks = [
        ModelBasedCollector(
            model_env=model_env, agent=agent, memory=memory, rollout_scheduler=BPStepScheduler(*[0, 10, rollout_size, rollout_size])
        ),
        BackPropStepsTerminator(
            bp_steps=1
        )
    ]
    fake_dataset = FakeDataloader(
        num_obs=num_obs, data_group='default', batch_size=batch_size
    )
    trainer = Trainer(callbacks, fake_dataset)
    trainer.train()

    if memory.size() != (rollout_size*batch_size):
        raise Exception(f"The RL replay buffer must contain rollout_size x batch_size "
                        f"= {rollout_size*batch_size} but it contains {memory.size()}")

    data_iter = iter(dataloader)
    batch = next(data_iter)

    if rl_data_group not in batch.keys():
        raise Exception(f"The RL data group does not exist in the keys\n")
    batch = batch[rl_data_group]

    obs = batch['observation']['obs']
    next_obs = batch['next_observation']['obs']
    actions = batch['actions']
    rewards = batch['rewards']
    model_in = torch.cat((obs, actions), dim=1)
    model_out = torch.cat((next_obs, rewards), dim=1)

    if torch.mean(torch.abs(rand_multiplier*model_in-model_out)) > 0.001:
        raise Exception(f"The loaded samples do not look correct\n")


if __name__ == "__main__":
    test_model_collector()
